---
layout: default
title: Redes Neurais PT-BR
description: Quest√µes de Redes Neurais (Conceitos b√°sicos)
---

Step Functions
+ Sigmoid Function
  - y entre [0,1]. O intervalo √© de 1
+ Hyper Bolian Tangent Function
  - y √© entre [-1,1]. O intervalo √© de 2
+ SoftMax function
  - Boa para distribui√ß√¢o de probabildiade (para classifica√ß√£o entre mais de duas classifica√ß√µes)
+ Rectified Linear Unit (ReLu)
  - Uma reta positiva que vai de [0, inf]. Se valor positivo, retorna esse avlor, se neagito, retorna 0.


+ Epochs
 - Unidade de treinamento da rede. Calcular a previs√¢o, verificar o erros e alterar os pesos pelo BackPropagation
+ HyperParametros: Parametro como (learning-rate (lr))


### Which optimization techniques for training neural nets do you know? ‚Äç‚≠êÔ∏è

Gradient Descent
Stochastic Gradient Descent
Mini-Batch Gradient Descent(best among gradient descents)
Nesterov Accelerated Gradient
Momentum
Adagrad
AdaDelta
Adam(best one. less time, more efficient)

### What‚Äôs the learning rate? üë∂

The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the ‚Äústep width‚Äù during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem.



### What happens when the learning rate is too large? Too small? üë∂

A large learning rate can accelerate the training. However, it is possible that we ‚Äúshoot‚Äù too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won‚Äôt update even if it is not the best possible global solution.

### What is Adam? What‚Äôs the main difference between Adam and SGD? ‚Äç‚≠êÔ∏è

Adam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don‚Äôt want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.

Adam tends to converge faster, while SGD often converges to more optimal solutions. SGD‚Äôs high variance disadvantages gets rectified by Adam (as advantage for Adam).



### When would you use Adam and when SGD? ‚Äç‚≠êÔ∏è

Adam tends to converge faster, while SGD often converges to more optimal solutions.

### How do we decide when to stop training a neural net? üë∂

Simply stop training when the validation error is the minimum.


