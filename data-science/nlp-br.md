### Embedding
Embeddings allow you to turn a categorical variable (e.g., words in a book) into a fixed size vector of real numbers. The key features of embeddings are that they:

map high dimensional data into a lower-dimensional space

can be trained to discover relationships between the data points (i.e., the vectors).

By transforming a categorical feature into a numeric value through embeddings, it can be used by your model either as a feature or target value.

Use embeddings to encode categorical features with a large number of categories (e.g., words or sentences) and/or when itâ€™s important to understand how different categories of your categorical feature relate to each other.

### LSTM unit
A long short term memory unit is a special kind of recurrent neural network building block that has a build in ability to 'remember' or 'forget' parts of sequential data. This ability allows a RNN using LSTM units to learn very long range connections in sequential data, by keeping relevant information 'stored' in the unit.

Use in recurrent neural networks.

### Natural language processing (NLP)
Natural language processing (NLP) techniques aim to automatically process, analyze and manipulate (large amounts) of language data like speech and text.